{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import glob\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = glob.glob(r'../input/lungdatasets3/lung2/label/train_label/*.png')\n",
    "counts=len(image)\n",
    "i = 1\n",
    "j = counts\n",
    "img = []\n",
    "label=[]\n",
    "while i <=j:\n",
    "    img_path=str('../input/lungdatasets3/lung2/image/train_image/'+str(i)+'.jpg')\n",
    "    i+=1\n",
    "    img.append(img_path)\n",
    "img= [str(path) for path in img]\n",
    "i =1\n",
    "j = counts\n",
    "while i <=j:\n",
    "    k=0\n",
    "    label_path=str('../input/lungdatasets3/lung2/label/train_label/'+str(i)+'.png')\n",
    "    i+=1\n",
    "    label.append(label_path)\n",
    "    k+=1\n",
    "label= [str(path) for path in label]\n",
    "val=glob.glob(r'../input/lungdatasets3/lung2/label/test_label/*.png')\n",
    "counts=len(val)\n",
    "i = 1\n",
    "j =counts\n",
    "img_val = []\n",
    "label_val=[]\n",
    "while i <j:\n",
    "    img_path=str('../input/lungdatasets3/lung2/image/test_image/'+str(i)+'.jpg')\n",
    "    i+=1\n",
    "    img_val.append(img_path)\n",
    "img_val= [str(path) for path in img_val]\n",
    "i =1\n",
    "j =counts\n",
    "while i <j:\n",
    "    k=0\n",
    "    label_path=str('../input/lungdatasets3/lung2/label/test_label/'+str(i)+'.png')\n",
    "    i+=1\n",
    "    label_val.append(label_path)\n",
    "    k+=1\n",
    "label_val= [str(path) for path in label_val]\n",
    "train_count =len(img)\n",
    "val_count = len(img_val)\n",
    "dataset_train = tf.data.Dataset.from_tensor_slices((img, label))\n",
    "dataset_val = tf.data.Dataset.from_tensor_slices((img_val, label_val))\n",
    "def read_jpg(path):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    return img\n",
    "def read_png_label(path):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_jpeg(img, channels=1)\n",
    "    return img\n",
    "def normal(img, mask):\n",
    "    img = tf.cast(img, tf.float32)/127.5 -1\n",
    "    mask = mask\n",
    "    mask = tf.cast(mask, tf.int32)\n",
    "    return img, mask\n",
    "                                                                                                                                                                                                                                             \n",
    "\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        img = tf.image.flip_left_right(img)\n",
    "        mask = tf.image.flip_left_right(mask)\n",
    "\n",
    "    img, mask = normal(img, mask)\n",
    "\n",
    "    return img, mask\n",
    "\n",
    "\n",
    "\n",
    "dataset_train = dataset_train.map(load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "dataset_val = dataset_val.map(load_image_val, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "BATCH_SIZE =4\n",
    "dataset_train =dataset_train.repeat(count=1).shuffle(50).batch(BATCH_SIZE)\n",
    "\n",
    "dataset_val =dataset_val.batch(1)\n",
    "\n",
    "for i, m in dataset_train.take(50):\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(tf.keras.preprocessing.image.array_to_img(i[0]))\n",
    "    plt.subplot(1, 2, 2)           \n",
    "    plt.imshow(tf.keras.preprocessing.image.array_to_img(m[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, m in dataset_train.take(50):\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(tf.keras.preprocessing.image.array_to_img(i[0]))\n",
    "    plt.subplot(1, 2, 2)           \n",
    "    plt.imshow(tf.keras.preprocessing.image.array_to_img(m[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitConv2(keras.layers.Layer):\n",
    "    r = 5\n",
    "    reduction_factor = 2\n",
    "    def __init__(self,units, kernel_size=3, strides=1,padding='same'):\n",
    "        super(SplitConv2, self).__init__()\n",
    "        self.conv = Conv2(units, kernel_size=3, strides=strides,padding='same')\n",
    "        self.conv4 = Conv2(units, kernel_size=3, strides=strides,padding='same')\n",
    "        self.conv5 = Conv2(units, kernel_size=3, strides=strides,padding='same')\n",
    "        self.conv6 = Conv2(units, kernel_size=3, strides=strides,padding='same')\n",
    "        self.conv7 = Conv2(units, kernel_size=3, strides=strides,padding='same')\n",
    "        self.pool = keras.layers.GlobalAveragePooling2D()\n",
    "        self.bn= keras.layers.BatchNormalization(momentum =0.9 ,epsilon =1e-5)\n",
    "        self.conv1 = keras.layers.Conv2D(units*3*3, kernel_size=1, padding='same')\n",
    "        self.conv2 = keras.layers.Conv2D(units*3*3, kernel_size=1, padding='same')\n",
    "        self.conv3 = keras.layers.Conv2D(units*3, kernel_size=1, padding='same')\n",
    "        self.conv8 = keras.layers.Conv2D(units*3//2, kernel_size=1, padding='same')\n",
    "        self.conv9= keras.layers.Conv2D(units*3, kernel_size=1, padding='same')\n",
    "        self.sigmoid = tf.keras.layers.Activation('softmax')\n",
    "        self.n = units\n",
    "        self.m = units\n",
    "        self.strides=strides\n",
    "        self.spatial =keras.layers.Conv2D(1,kernel_size = 7,strides=1,padding='same')\n",
    "    def call(self,x):\n",
    "        n = self.n\n",
    "        m = self.m*3\n",
    "        strides =self.strides\n",
    "\n",
    "        k=tf.split(x,2,axis=-1)\n",
    "        k0=self.conv3(k[0])\n",
    "            \n",
    "        x1=tf.split(k0,3,axis=-1)\n",
    "        x10 = self.conv(x1[0])\n",
    "        x11 = self.conv4(x1[1])\n",
    "        x12 = self.conv5(x1[2])\n",
    "        x_origin = x10\n",
    "        x1 = tf.add_n([x10,x11,x12])\n",
    "        x1 = self.pool(x1)\n",
    "        x1 = tf.expand_dims(x1,axis=1)\n",
    "        x1 = tf.expand_dims(x1,axis=1)\n",
    "        if strides ==2:\n",
    "            x1 = self.conv8(x1)\n",
    "        else:\n",
    "            x1 = self.conv1(x1)\n",
    "        x1 = self.bn(x1)\n",
    "        x1 = tf.nn.relu(x1)\n",
    "        if strides ==2:\n",
    "            m=self.m\n",
    "            x1 = self.conv9(x1)\n",
    "        else:\n",
    "            x1 = self.conv2(x1)\n",
    "        x1 = self.bn(x1)\n",
    "        x1 = tf.nn.relu(x1)\n",
    "        x1 = self.sigmoid(x1)\n",
    "        x1= tf.keras.layers.UpSampling2D(size=(x_origin.shape[1],x_origin.shape[2]),interpolation = 'nearest')(x1)\n",
    "        s=tf.split(x1,3,axis=-1)\n",
    "        x_10=tf.multiply(s[0],x10)\n",
    "        x_11=tf.multiply(s[1],x11)\n",
    "        x_12=tf.multiply(s[2],x12)\n",
    "        #x_3=tf.multiply(s[3],x3)\n",
    "       # x_4=tf.multiply(s[4],x4)\n",
    "        avgpool10 = tf.reduce_mean(x_10,axis=3,keepdims=True )\n",
    "        maxpool10 = tf.reduce_max(x_10,axis=3,keepdims=True )\n",
    "        spatial10 = tf.concat([avgpool10,maxpool10],axis=3)\n",
    "        spatial10 = self.spatial(spatial10)\n",
    "        spatial10 =self.sigmoid(spatial10)\n",
    "        spatial11 = self.spatial(spatial11)\n",
    "        spatial11 =self.sigmoid(spatial11)\n",
    "        x_11  =tf.multiply(x_11,spatial11)\n",
    "        avgpool12 = tf.reduce_mean(x_12,axis=3,keepdims=True )\n",
    "        maxpool12 = tf.reduce_max(x_12,axis=3,keepdims=True )\n",
    "        spatial12 = tf.concat([avgpool12,maxpool12],axis=3)\n",
    "        spatial12 = self.spatial(spatial12)\n",
    "        spatial12 =self.sigmoid(spatial12)\n",
    "        x_12  =tf.multiply(x_12,spatial12)\n",
    "        x1=tf.add_n([x_10,x_11,x_12])\n",
    "\n",
    "\n",
    "        #x3 = self.conv6(x[3])\n",
    "        \n",
    "        k1=self.conv3(k[1])\n",
    "        \n",
    "        x2=tf.split(k1,3,axis=-1)\n",
    "        x20 = self.conv(x2[0])\n",
    "        x21 = self.conv4(x2[1])\n",
    "        x22 = self.conv5(x2[2])\n",
    "        x_origin = x20\n",
    "        x2 = tf.add_n([x20,x21,x22])\n",
    "        x2 = self.pool(x2)\n",
    "        x2 = tf.expand_dims(x2,axis=1)\n",
    "        x2 = tf.expand_dims(x2,axis=1)\n",
    "        if strides ==2:\n",
    "            x2 = self.conv8(x2)\n",
    "        else:\n",
    "            x2 = self.conv1(x2)\n",
    "        x2 = self.bn(x2)\n",
    "        x2 = tf.nn.relu(x2)\n",
    "        if strides ==2:\n",
    "            m=self.m\n",
    "            x2 = self.conv9(x2)\n",
    "        else:\n",
    "            x2 = self.conv2(x2)\n",
    "        x2 = self.bn(x2)\n",
    "        x2 = tf.nn.relu(x2)\n",
    "        x2 = self.sigmoid(x2)\n",
    "        x2= tf.keras.layers.UpSampling2D(size=(x_origin.shape[1],x_origin.shape[2]),interpolation = 'nearest')(x2)\n",
    "        s2=tf.split(x2,3,axis=-1)\n",
    "        x_20=tf.multiply(s2[0],x20)\n",
    "        x_21=tf.multiply(s2[1],x21)\n",
    "        x_22=tf.multiply(s2[2],x22)\n",
    "        #x_3=tf.multiply(s[3],x3)\n",
    "       # x_4=tf.multiply(s[4],x4)\n",
    "        avgpool20 = tf.reduce_mean(x_20,axis=3,keepdims=True )\n",
    "        maxpool20 = tf.reduce_max(x_20,axis=3,keepdims=True )\n",
    "        spatial20 = tf.concat([avgpool20,maxpool20],axis=3)\n",
    "        spatial20 = self.spatial(spatial20)\n",
    "        spatial20 =self.sigmoid(spatial20)\n",
    "        x_20  =tf.multiply(x_20,spatial20)\n",
    "        avgpool21 = tf.reduce_mean(x_21,axis=3,keepdims=True )\n",
    "        maxpool21 = tf.reduce_max(x_21,axis=3,keepdims=True )\n",
    "        spatial21 = tf.concat([avgpool21,maxpool21],axis=3)\n",
    "        spatial21 = self.spatial(spatial21)\n",
    "        spatial21 =self.sigmoid(spatial21)\n",
    "        x_21  =tf.multiply(x_21,spatial21)\n",
    "        avgpool22 = tf.reduce_mean(x_22,axis=3,keepdims=True )\n",
    "        maxpool22 = tf.reduce_max(x_22,axis=3,keepdims=True )\n",
    "        spatial22 = tf.concat([avgpool22,maxpool22],axis=3)\n",
    "        spatial22 = self.spatial(spatial22)\n",
    "        spatial22 =self.sigmoid(spatial22)\n",
    "        x_22  =tf.multiply(x_22,spatial22)\n",
    "       # avgpool3 = tf.reduce_mean(x_3,axis=3,keepdims=True )\n",
    "        #maxpool3 = tf.reduce_max(x_3,axis=3,keepdims=True )\n",
    "        #spatial3 = tf.concat([avgpool3,maxpool3],axis=3)\n",
    "        #spatial3 = self.spatial(spatial3)\n",
    "        #spatial3 =self.sigmoid(spatial3)\n",
    "        #x_3  =tf.multiply(x_3,spatial3)\n",
    "        #avgpool4 = tf.reduce_mean(x_4,axis=3,keepdims=True )\n",
    "        #maxpool4 = tf.reduce_max(x_4,axis=3,keepdims=True )\n",
    "        #spatial4 = tf.concat([avgpool4,maxpool4],axis=3)\n",
    "        #spatial4 = self.spatial(spatial4)\n",
    "        #spatial4 =self.sigmoid(spatial4)\n",
    "        #x_4  =tf.multiply(x_4,spatial4)\n",
    "        \n",
    "        x2=tf.add_n([x_20,x_21,x_22])\n",
    "        x =tf.concat([x2,x1],axis=-1)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(keras.layers.Layer):\n",
    "    def __init__(self,units,strides=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = SplitConv2(units, kernel_size=3, strides=strides,padding='same') \n",
    "        self.conv2 = keras.layers.Conv2D(units*2, kernel_size=1, padding='same') \n",
    "        self.bn2 = keras.layers.BatchNormalization(momentum =0.9 ,epsilon =1e-5)\n",
    "        self.downsample = keras.layers.Conv2D(units*2, kernel_size=1,strides=1, padding='same')\n",
    "        self.pool=tf.keras.layers.AveragePooling2D(pool_size=(2,2),strides=strides,padding='same')\n",
    "    def call(self,x,is_downsample=False):\n",
    "        identity = x\n",
    "        if is_downsample:\n",
    "            identity =self.pool(x)\n",
    "            identity = self.downsample(identity)\n",
    "        x = self.conv1(x)\n",
    "         \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        \n",
    "        x = tf.add(x,identity)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Downsample(keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(Downsample, self).__init__()\n",
    "        self.conv1 = keras.layers.Conv2D(units, kernel_size=3, padding='same')\n",
    "        self.conv2 = keras.layers.Conv2D(units, kernel_size=3, padding='same')\n",
    "        self.pool = keras.layers.MaxPooling2D()\n",
    "        \n",
    "    def call(self, x, is_pool=True):\n",
    "        if is_pool:\n",
    "            x = self.pool(x)\n",
    "        x = self.conv1(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Upsample(keras.layers.Layer):\n",
    "    def __init__(self, unit):\n",
    "        super(Upsample, self).__init__()\n",
    "        self.conv1 = keras.layers.Conv2D(unit, kernel_size=3, padding='same')\n",
    "        self.conv2 = keras.layers.Conv2D(unit, kernel_size=3, padding='same')\n",
    "        self.deconv = keras.layers.Conv2DTranspose(unit//2,\n",
    "                                                   kernel_size=3,\n",
    "                                                   strides=2,\n",
    "                                                   padding='same')\n",
    "    def call(self, x, is_pool=True):\n",
    "        x = self.conv1(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.deconv(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.input_conv1 = keras.layers.Conv2D(64, kernel_size=3, strides=1,padding='same')\n",
    "        self.input_conv2 = keras.layers.Conv2D(64, kernel_size=3, strides=1,padding='same')\n",
    "        \n",
    "        self.down1_1 = BasicBlock(64,strides = 2)\n",
    "        self.down1_2 = BasicBlock(64,strides = 1)\n",
    "        self.down1_3 = BasicBlock(64,strides = 1)\n",
    "        \n",
    "        self.down2_1 = BasicBlock(128,strides = 2)\n",
    "        self.down2_2 = BasicBlock(128,strides = 1)\n",
    "        self.down2_3 = BasicBlock(128,strides = 1)\n",
    "        self.down2_4 = BasicBlock(128,strides = 1) \n",
    "        \n",
    "        self.down3_1 = BasicBlock(256,strides = 2)\n",
    "        self.down3_2 = BasicBlock(256,strides = 1)\n",
    "        self.down3_3 = BasicBlock(256,strides = 1)\n",
    "        self.down3_4 = BasicBlock(256,strides = 1)\n",
    "        self.down3_5 = BasicBlock(256,strides = 1)\n",
    "        self.down3_6 = BasicBlock(256,strides = 1)\n",
    "        \n",
    "        self.down4_1 = BasicBlock(512,strides = 2)\n",
    "        self.down4_2 = BasicBlock(512,strides = 1)\n",
    "        self.down4_3 = BasicBlock(512,strides = 1)\n",
    "        \n",
    "        \n",
    "    \n",
    "        #self.conv = tf.keras.layers.Conv2D(512,kernel_size=3,strides = 1,padding = 'same')\n",
    "       \n",
    "        self.up1 = Upsample(512)\n",
    "        self.up2 = Upsample(256)\n",
    "        self.up3 = Upsample(128)\n",
    "     \n",
    "        \n",
    "        \n",
    "        self.up = keras.layers.Conv2DTranspose(512,\n",
    "                                               kernel_size=3,\n",
    "                                               strides = 2,\n",
    "                                               padding='same')\n",
    "        self.conv_last = Downsample(64)\n",
    "        \n",
    "        \n",
    "        self.last = keras.layers.Conv2D(2,kernel_size=1,strides =1,padding='same')\n",
    "        \n",
    "        self.convdown1_2 =keras.layers.Conv2D(128,kernel_size=1,strides=2,padding='same')\n",
    "        self.convdown1_3 =keras.layers.Conv2D(256,kernel_size=1,strides=4,padding='same')\n",
    "        self.convdown1_4 =keras.layers.Conv2D(512,kernel_size=1,strides=8,padding='same')\n",
    "        \n",
    "        self.convup2_1  =keras.layers.Conv2DTranspose(64,kernel_size=3,strides=2,padding='same')\n",
    "        self.convdown2_3 =keras.layers.Conv2D(256,kernel_size=1,strides=2,padding='same')\n",
    "        self.convdown2_4 =keras.layers.Conv2D(512,kernel_size=1,strides=4,padding='same')\n",
    "        \n",
    "        self.convup3_1  =keras.layers.Conv2DTranspose(64,kernel_size=3,strides=4,padding='same')\n",
    "        self.convup3_2  =keras.layers.Conv2DTranspose(128,kernel_size=3,strides=2,padding='same')\n",
    "        self.convdown3_4 =keras.layers.Conv2D(512,kernel_size=1,strides=2,padding='same')\n",
    "        \n",
    "        self.convup4_1  =keras.layers.Conv2DTranspose(64,kernel_size=3,strides=8,padding='same')\n",
    "        self.convup4_2  =keras.layers.Conv2DTranspose(128,kernel_size=3,strides=4,padding='same')\n",
    "        self.convup4_3  =keras.layers.Conv2DTranspose(256,kernel_size=3,strides=2,padding='same')\n",
    "        \n",
    "  \n",
    "        self.convcat1 = keras.layers.Conv2D(64,kernel_size=1,strides=1,padding='same')\n",
    "        self.convcat1_2 = keras.layers.Conv2D(64,kernel_size=1,strides=1,padding='same')\n",
    "        self.bn1_1 = keras.layers.BatchNormalization(momentum =0.9 ,epsilon =1e-5)\n",
    "        self.bn1_2 = keras.layers.BatchNormalization(momentum =0.9 ,epsilon =1e-5)\n",
    "        \n",
    "        self.convcat2 = keras.layers.Conv2D(128,kernel_size=1,strides=1,padding='same')\n",
    "        self.convcat2_2 = keras.layers.Conv2D(128,kernel_size=1,strides=1,padding='same')\n",
    "        self.bn2_1 = keras.layers.BatchNormalization(momentum =0.9 ,epsilon =1e-5)\n",
    "        self.bn2_2= keras.layers.BatchNormalization(momentum =0.9 ,epsilon =1e-5)\n",
    "        \n",
    "        self.convcat3 = keras.layers.Conv2D(256,kernel_size=1,strides=1,padding='same')\n",
    "        self.convcat3_2 = keras.layers.Conv2D(256,kernel_size=1,strides=1,padding='same')\n",
    "        self.bn3_1 = keras.layers.BatchNormalization(momentum =0.9 ,epsilon =1e-5)\n",
    "        self.bn3_2 = keras.layers.BatchNormalization(momentum =0.9 ,epsilon =1e-5)\n",
    "        \n",
    "        self.convcat4 = keras.layers.Conv2D(512,kernel_size=1,strides=1,padding='same')\n",
    "        self.convcat4_2 = keras.layers.Conv2D(512,kernel_size=1,strides=1,padding='same')\n",
    "        self.bn4_1 = keras.layers.BatchNormalization(momentum =0.9 ,epsilon =1e-5)\n",
    "        self.bn4_2 = keras.layers.BatchNormalization(momentum =0.9 ,epsilon =1e-5)\n",
    "        \n",
    "        \n",
    "        self.enhance = Enhance(units=256,padding='same')\n",
    "        self.conv_e = tf.keras.layers.Conv2D(1024,kernel_size=1,strides=1,padding='same')\n",
    "        self.bn1= keras.layers.BatchNormalization(momentum =0.9 ,epsilon =1e-5)\n",
    "\n",
    "    def call(self,x):\n",
    "        x1 =self.input_conv1(x)  #256*256*64\n",
    "        x1 =self.input_conv2(x1)  #256*256*64\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        x2 = self.down1_1(x1,is_downsample = True)  #128*128*128\n",
    "        x2 = self.down1_2(x2)\n",
    "        x2 = self.down1_3(x2)\n",
    "       \n",
    "        \n",
    "        \n",
    "        x3 = self.down2_1(x2,is_downsample = True)  #64*64*256\n",
    "        x3 = self.down2_2(x3)\n",
    "        x3 = self.down2_3(x3)\n",
    "        x3 = self.down2_4(x3)\n",
    "        \n",
    "       \n",
    "        \n",
    "        x4 = self.down3_1(x3,is_downsample = True) #32*32*512\n",
    "        x4 = self.down3_2(x4)\n",
    "        x4 = self.down3_3(x4)\n",
    "        x4 = self.down3_4(x4)\n",
    "        x4 = self.down3_5(x4)\n",
    "        x4 = self.down3_6(x4)\n",
    "\n",
    "       \n",
    "        x5 = self.down4_1(x4,is_downsample = True) #16*16*1024\n",
    "        x5 = self.down4_2(x5)\n",
    "        x5 = self.down4_3(x5)\n",
    "            \n",
    "        x5_e = self.enhance(x5)  \n",
    "                                              \n",
    "        #x5_4 =self.convdown5_4(x5_e) \n",
    "        #x5_3 =self.convdown5_3(x5_e)                                       \n",
    "        #x5_2 =self.convdown5_2(x5_e)\n",
    "        #x5_1 =self.convdown5_1(x5_e)                                     \n",
    "                                              \n",
    "        x5 = tf.concat([x5,x5_e],axis = -1)\n",
    "        x5 = tf.nn.relu(x5)\n",
    "        x5 = self.conv_e(x5)\n",
    "        x5 = self.bn1(x5)\n",
    "        x5 =tf.nn.relu(x5)\n",
    "        \n",
    "        x5 = self.up(x5)   \n",
    "                \n",
    "        x1_4 =self.convdown1_4(x1)            \n",
    "        x2_4 =self.convdown2_4(x2)\n",
    "        x3_4 =self.convdown3_4(x3)\n",
    "        x4_s =tf.concat([x1_4,x2_4,x3_4],axis=-1)\n",
    "        x4_s =tf.nn.relu(x4_s)\n",
    "        x4_s =self.convcat4(x4_s)\n",
    "        x4_s =self.bn4_1(x4_s)\n",
    "        x4 =tf.concat([x4_s,x4],axis=-1)\n",
    "        x4 =self.convcat4_2(x4)\n",
    "        x4 = self.bn4_2(x4)\n",
    "        x5 = tf.concat([x4,x5], axis=-1)      #32*32*1024\n",
    "        x5 =tf.nn.relu(x5)\n",
    "        \n",
    "        x5 = self.up1(x5)     #32*32*512\n",
    "        \n",
    "        x1_3 =self.convdown1_3(x1)            \n",
    "        x2_3 =self.convdown2_3(x2)\n",
    "        x4_3 =self.convup4_3(x4)\n",
    "        x3_s =tf.concat([x1_3,x2_3,x4_3],axis=-1)\n",
    "        x3_s =tf.nn.relu(x3_s)\n",
    "        x3_s =self.convcat3(x3_s)\n",
    "        x3_s =self.bn3_1(x3_s)\n",
    "        x3 =tf.concat([x3_s,x3],axis=-1)\n",
    "        x3 =self.convcat3_2(x3)\n",
    "        x3 =self.bn3_2(x3)\n",
    "        x5 = tf.concat([x3,x5], axis=-1)      #64*64*512\n",
    "        x5 =tf.nn.relu(x5)\n",
    "        \n",
    "        x5 = self.up2(x5)                     #128*128*128\n",
    "        \n",
    "        x1_2 =self.convdown1_2(x1)            \n",
    "        x3_2 =self.convup3_2(x3)\n",
    "        x4_2 =self.convup4_2(x4)\n",
    "        x2_s =tf.concat([x1_2,x3_2,x4_2],axis=-1)\n",
    "        x2_s =tf.nn.relu(x2_s)\n",
    "        x2_s =self.convcat2(x2_s)\n",
    "        x2_s =self.bn2_1(x2_s)\n",
    "        x2 =tf.concat([x2_s,x2],axis=-1)\n",
    "        x2 =self.convcat2_2(x2)\n",
    "        x2 =self.bn2_2(x2)\n",
    "        x5 = tf.concat([x2,x5], axis=-1)      #128*128*256\n",
    "        x5 =tf.nn.relu(x5)\n",
    "        \n",
    "        x5 = self.up3(x5)                     #256*256*64\n",
    "        \n",
    "        x2_1 =self.convup2_1(x2)            \n",
    "        x3_1 =self.convup3_1(x3)\n",
    "        x4_1 =self.convup4_1(x4)\n",
    "        x1_s =tf.concat([x2_1,x3_1,x4_1],axis=-1)\n",
    "        x1_s =tf.nn.relu(x1_s)\n",
    "        x1_s =self.convcat1(x1_s)\n",
    "        x1_s =self.bn1_1(x1_s)\n",
    "        x1 =tf.concat([x1_s,x1],axis=-1)\n",
    "        x1 =self.convcat1_2(x1)\n",
    "        x1 =self.bn1_2(x1)\n",
    "        x5 = tf.concat([x1,x5], axis=-1)      #256*256*128\n",
    "        x5 =tf.nn.relu(x5)\n",
    "        \n",
    "          \n",
    "        x5 = self.conv_last(x5, is_pool=False)  #256*256*64\n",
    "        \n",
    "        x6 = self.last(x5)                     #256*256*2\n",
    "        \n",
    "        \n",
    "        return x6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dice_coef(tf.keras.metrics.Metric):\n",
    "    def __init__(self):\n",
    "        super(Dice_coef, self).__init__()\n",
    "        self.total =self.add_weight(name='total',dtype=tf.int32,initializer =tf.zeros_initializer())\n",
    "        self.count =self.add_weight(name='count',dtype=tf.int32,initializer =tf.zeros_initializer())\n",
    "        self.count2 =self.add_weight(name='count2',dtype=tf.int32,initializer =tf.zeros_initializer())\n",
    "    def update_state(self,y_true, y_pred,sample_weight=None):   \n",
    "        y_pred = tf.argmax(y_pred, axis=-1,output_type=tf.int32)\n",
    "        y_true_f = tf.keras.backend.flatten(y_true) # 将 y_true 拉伸为一维.\n",
    "        y_pred_f = tf.keras.backend.flatten(y_pred)\n",
    "    #print(y_pred_f.shape, y_true_f.shape)\n",
    "        values =tf.keras.backend.sum(y_true_f * y_true_f)\n",
    "        values2=tf.keras.backend.sum(y_pred_f * y_pred_f)\n",
    "        intersection =tf.keras.backend.sum(y_true_f * y_pred_f)\n",
    "        intersection =2*intersection+1\n",
    "        self.count2.assign_add(intersection)\n",
    "        self.total.assign_add(values)\n",
    "        self.count.assign_add(values2)\n",
    "    def result(self):\n",
    "        return self.count2/ (self.total + self.count + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanIoU(tf.keras.metrics.MeanIoU):\n",
    "    def __call__(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.argmax(y_pred, axis=-1)\n",
    "        return super().__call__(y_true, y_pred, sample_weight=sample_weight)\n",
    "learning_rate_fn = tf.keras.optimizers.schedules.PiecewiseConstantDecay([851/4*5,851/4*30,851/4*50],[0.0002,0.00015,0.0001,0.0001*0.5])\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate_fn)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "train_iou = MeanIoU(34, name='train_iou')\n",
    "\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
    "test_iou = MeanIoU(34, name='test_iou')\n",
    " \n",
    "test_recall=Recall()\n",
    "test_precision=Precision()\n",
    "test_dice =Dice_coef()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "    train_iou(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    predictions = model(images)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)\n",
    "    test_iou(labels, predictions)\n",
    "    test_dice.update_state(labels, predictions)\n",
    "    test_recall.update_state(labels, predictions)\n",
    "    test_precision.update_state(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_results = []\n",
    "train_accuracy_results = []\n",
    "train_iou_results = []\n",
    "\n",
    "test_loss_results = []\n",
    "test_accuracy_results = []\n",
    "test_iou_result = []\n",
    "train_loss_results = []\n",
    "train_accuracy_results = []\n",
    "train_iou_results = []\n",
    "\n",
    "test_loss_results = []\n",
    "test_accuracy_results = []\n",
    "test_iou_result = []\n",
    "test_recall_results =[]\n",
    "test_precision_results =[]\n",
    "test_dice_results =[]\n",
    "test_f1_socre_results =[]\n",
    "test_acc_results =[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS =70\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    sleep(0.01)\n",
    "    # 在下一个epoch开始时，重置评估指标\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    train_iou.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "    test_iou.reset_states()\n",
    "    test_dice.reset_states()\n",
    "    test_recall.reset_states()\n",
    "    test_precision.reset_states()\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    train_loss_results.append(train_loss.result())\n",
    "    train_accuracy_results.append(train_accuracy.result())\n",
    "    train_iou_results.append(train_iou.result())\n",
    "\n",
    "    for test_images, test_labels in dataset_val:\n",
    "        test_step(test_images, test_labels)\n",
    "        #test_recall_results.append(sensitivity(labels,images))\n",
    "        #test_precision_results.append(precision(labels,images))\n",
    "        #test_dice_results.append(dice_coef(labels[i],images[i]))\n",
    "        #test_f1_socre_results.append(f1_socre(labels,images))\n",
    "    test_loss_results.append(test_loss.result())\n",
    "    test_iou_result.append(test_iou.result())\n",
    "    test_accuracy_results.append(test_accuracy.result())\n",
    "    test_dice_results.append(test_dice.result())\n",
    "    test_recall_results.append(test_recall.result())\n",
    "    test_precision_results.append(test_precision.result())\n",
    "\n",
    "    template = 'Epoch {:.3f}, Loss: {:.5f}, Accuracy: {:.3f}, \\\n",
    "                IOU: {:.3f}, Test Loss: {:.5f}, \\\n",
    "                Test Accuracy: {:.3f}, Test IOU: {:.5f},Test Dice: {:.5f},Test Recall: {:.5f},Test Precision: {:.5f},'\n",
    "    print (template.format(epoch+1,\n",
    "                           train_loss.result(),\n",
    "                           train_accuracy.result()*100,\n",
    "                           train_iou.result(),\n",
    "                           test_loss.result(),\n",
    "                           test_accuracy.result()*100,\n",
    "                           test_iou.result(),\n",
    "                           test_dice.result(),\n",
    "                           test_recall.result(),\n",
    "                           test_precision.result()\n",
    "                           ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(test_iou_result)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a  = np.array(test_loss_results)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a  = np.array(train_iou_results)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a  = np.array(train_loss_results)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a  = np.array(test_dice_results)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('./rem')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "../input/lungdatasets3/lung2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    for image,mask in dataset_val.take(i):\n",
    "        pred_mask = model.predict(image)\n",
    "        pred_mask = tf.argmax(pred_mask,axis=-1)\n",
    "        pred_mask = pred_mask[...,tf.newaxis]\n",
    "        filename1 =str('./rem/img'+str(i)+'.png')\n",
    "        filename2 =str('./rem/label'+str(i)+'.png')\n",
    "        filename3 =str('../rem/pre'+str(i)+'.png')\n",
    "        a = np.array(tf.keras.preprocessing.image.array_to_img(image[0]))\n",
    "        b = np.array(tf.keras.preprocessing.image.array_to_img(mask[0]))\n",
    "        c = np.array(tf.keras.preprocessing.image.array_to_img(pred_mask[0]))\n",
    "        cv2.imwrite(filename1,a)\n",
    "        cv2.imwrite(filename2,b)\n",
    "        cv2.imwrite(filename3,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image,mask in dataset_val.take(127):\n",
    "    pred_mask = model.predict(image)\n",
    "    pred_mask = tf.argmax(pred_mask,axis=-1)\n",
    "    pred_mask = pred_mask[...,tf.newaxis]\n",
    "num = 1\n",
    "plt.figure(figsize=(25,25))\n",
    "for i in range (num):\n",
    "    plt.subplot(num,6,i*num+1)\n",
    "    plt.imshow(tf.keras.preprocessing.image.array_to_img(image[i]))\n",
    "    plt.subplot(num,6,i*num+2)\n",
    "    plt.imshow(tf.keras.preprocessing.image.array_to_img(mask[i]))\n",
    "    plt.subplot(num,6,i*num+3)\n",
    "    plt.imshow(tf.keras.preprocessing.image.array_to_img(pred_mask[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image,mask in dataset_val.take(107):\n",
    "    pred_mask = model.predict(image)\n",
    "    pred_mask = tf.argmax(pred_mask,axis=-1)\n",
    "    pred_mask = pred_mask[...,tf.newaxis]\n",
    "    a = np.array(tf.keras.preprocessing.image.array_to_img(image[0]))\n",
    "    b = np.array(tf.keras.preprocessing.image.array_to_img(mask[0]))\n",
    "    c = np.array(tf.keras.preprocessing.image.array_to_img(pred_mask[0]))\n",
    "    filename1 =str('./img'+str(107)+'.png')\n",
    "    filename2 =str('./label'+str(107)+'.png')\n",
    "    filename3 =str('./pre'+str(107)+'.png')\n",
    "    cv2.imwrite(filename1,a)\n",
    "    cv2.imwrite(filename2,b)\n",
    "    cv2.imwrite(filename3,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.preprocessing.image.array_to_img(pred_mask[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= tf.keras.preprocessing.image.array_to_img(pred_mask[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite('./2.jpg',a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b =0\n",
    "for i in range(len(test_recall_results)):\n",
    "    c=i+1\n",
    "    a =test_recall_results[i]\n",
    "    b =b+a\n",
    "    if c %370==0:\n",
    "        b =b/370\n",
    "        print(b)\n",
    "        b=0\n",
    "b =0\n",
    "for i in range(len(test_precision_results)):\n",
    "    c=i+1\n",
    "    a =test_precision_results[i]\n",
    "    b =b+a\n",
    "    if c %369==0:\n",
    "        b =b/369\n",
    "        print(b)\n",
    "        b=0\n",
    "b =0\n",
    "for i in range(len(test_f1_socre_results)):\n",
    "    c=i+1\n",
    "    a =test_f1_socre_results[i]\n",
    "    b =b+a\n",
    "    if c %369==0:\n",
    "        b =b/369\n",
    "        print(b)\n",
    "        b=0\n",
    "b =0\n",
    "for i in range(len(test_dice_results)):\n",
    "    c=i+1\n",
    "    a =test_dice_results[i]\n",
    "    b =b+a\n",
    "    if c %369==0:\n",
    "        b =b/369\n",
    "        print(b)\n",
    "        b=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dice_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
